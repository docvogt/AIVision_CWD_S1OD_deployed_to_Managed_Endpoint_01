# source of the upper code - https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-online-endpoints?view=azureml-api-2&tabs=azure-studio
# but this code example presumes a .pkl model file, which, is Not the .onnx model we Do have...
# import os
# import logging
# import json
# import numpy
# import joblib


# def init():
#     """
#     This function is called when the container is initialized/started, typically after create/update of the deployment.
#     You can write the logic here to perform init operations like caching the model in memory
#     """
#     global model
#     # AZUREML_MODEL_DIR is an environment variable created during deployment.
#     # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)
#     # Please provide your model's folder name if there is one
#     model_path = os.path.join(
#         os.getenv("AZUREML_MODEL_DIR"), "model/sklearn_regression_model.pkl"
#     )
#     # deserialize the model file back into a sklearn model
#     model = joblib.load(model_path)
#     logging.info("Init complete")


# def run(raw_data):
#     """
#     This function is called for every invocation of the endpoint to perform the actual scoring/prediction.
#     In the example we extract the data from the json input and call the scikit-learn model's predict()
#     method and return the result back
#     """
#     logging.info("model 1: request received")
#     data = json.loads(raw_data)["data"]
#     data = numpy.array(data)
#     result = model.predict(data)
#     logging.info("Request processed")
#     return result.tolist()



# source of lower code - https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/deployment/onnx/onnx-train-pytorch-aml-deploy-mnist.ipynb
# THIS example was for an .onnx custom vision model...
# %%writefile score.py
import json
import time
import sys
import os
from azureml.core.model import Model
import numpy as np    # we're going to use numpy to process input and output data
import onnxruntime    # to inference ONNX models, we use the ONNX Runtime

#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
def init():
    global session
    # AZUREML_MODEL_DIR is an environment variable created during deployment.
    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)  # <--- HOW do models get IN this folder?!?!?!!
    # does this get created ON THE NEW HOST?? and the populated from the Registration?!?!?!  that would make sense, but where does that copying happen?!?!?!?!?

    # For multiple models, it points to the folder containing all deployed models (./azureml-models)
#    model = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'mnist.onnx')  # mnist.onnx model SHOULD be taking in imagery inputs of characters... small, but still images...

    model = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'model.onnx')   # mike's CWD model will be in the local folder defined by the global AZUREML_MODEL_DIR                   
                                                                         # "C:\Development\GitHub\AIVisionPrototype2\NADeerCWDobjectdetector.ONNX" / model.onnx
# mike note - how could this Ever have worked??? this code is running under a local Ubuntu OS, but the Azure ML Studio UI asks mike to locate his model from inside the local
# development platform's Windows11Pro OS.   The path name this constructs has nothing to do with the pathname on the chosen host being built


#    session = onnxruntime.InferenceSession(model)  # this generated ORT1.9 error bc did not explicitly state execution providers in order of selection if more than one is available... 
    session = onnxruntime.InferenceSession(
        model, providers=['CUDAExecutionProvider', 'CPUExecutionProvider']  # this should get past that error, will leave in place for when CUDA/GPU might be available, if not, will move to CPU...
)
#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

def preprocess(input_data_json):
    # convert the JSON data into the tensor input
    return np.array(json.loads(input_data_json)['data']).astype('float32')

def postprocess(result):
    # We use argmax to pick the highest confidence label
    return int(np.argmax(np.array(result).squeeze(), axis=0))

def run(input_data_json):
    try:
        start = time.time()   # start timer
        input_data = preprocess(input_data_json)
        input_name = session.get_inputs()[0].name  # get the id of the first input of the model     #???  model is only expecting a single input - name of a local image file...  
        result = session.run([], {input_name: input_data})
        end = time.time()     # stop timer
        return {"result": postprocess(result),
                "time": end - start}
    except Exception as e:
        result = str(e)
        return {"error": result}

# ------------------------------------------ end of code ------------------------------------------------